{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from sklearn import naive_bayes\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/Rutherford/Desktop/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path,'dataset.csv'),header=None)\n",
    "df.columns=['error','s1','s2']\n",
    "df.loc[df['s1'].isnull(),'s1']=''\n",
    "df.loc[df['s2'].isnull(),'s2']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#f_ratio,f_partial,f_sort,f_set = [],[],[],[]\n",
    "#for i in range(len(df)):\n",
    "#    f_ratio.append(fuzz.ratio(df['s1'][i],df['s2'][i]))\n",
    "#    f_partial.append(fuzz.ratio(df['s1'][i],df['s2'][i]))\n",
    "#    f_sort.append(fuzz.ratio(df['s1'][i],df['s2'][i]))\n",
    "#    f_set.append(fuzz.ratio(df['s1'][i],df['s2'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['f_ratio'] = f_ratio\n",
    "#df['f_partial'] = f_partial\n",
    "#df['f_sort'] = f_sort\n",
    "#df['f_set'] = f_set\n",
    "\n",
    "df['combo'] = df['s1']+','+df['s2']\n",
    "\n",
    "#shuffle\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "#X = df[['f_ratio','f_partial','f_sort','f_set']]\n",
    "#Y = df.pop('error')\n",
    "Y = np.where(Y==1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"df = pd.read_csv(os.path.join(data_path,'dataset.csv'),header=None)\n",
    "df.columns=['error','s1','s2']\n",
    "df.loc[df['s1'].isnull(),'s1']=''\n",
    "df.loc[df['s2'].isnull(),'s2']=''\n",
    "#df['combo'] = df['s1']+','+df['s2']\n",
    "df = df.sample(frac=1)\n",
    "Y = df.pop('error')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = .6\n",
    "val_size = .2\n",
    "test_size = .1\n",
    "\n",
    "val_test_split = int(len(Y)*val_size)\n",
    "test_train_split = int(len(Y)*test_size)+val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "x_val,x_test,x_train = \\\n",
    "X[:val_test_split],X[val_test_split:test_train_split],X[test_train_split:]\n",
    "\n",
    "y_val,y_test,y_train = \\\n",
    "Y[:val_test_split],Y[val_test_split:test_train_split],Y[test_train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "#tf_idf = TfidfVectorizer()\n",
    "\n",
    "x_train_cts = cv.fit_transform(x_train['s1'],x_train['s2'])#cv.fit_transform(x_train['combo'])\n",
    "x_val_cts = cv.transform(x_val['combo']) #cv.transform(x_val['combo'])\n",
    "#x_val_s2 = cv.transform(x_val['s2'])\n",
    "\n",
    "#x_train_cts = tf_idf.fit_transform(x_train['combo'])\n",
    "#x_val_cts = tf_idf.transform(x_val['combo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nb = naive_bayes.BernoulliNB().fit(x_train,y_train)\n",
    "#predicted = nb.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13266,    55],\n",
       "       [ 1588,    10]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st1 = \"a fngargle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S1_test = check.embeddings[[check.vocab_dict[w] for w in st1.split() if w in check.vocab_dict]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 300)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import euclidean_distances,confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from pyemd import emd\n",
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "class error_checker():\n",
    "    \"\"\"\n",
    "    Error checker class that builds embeddings upon instantiation, is capable\n",
    "    of being retrained, making predictions, and inspecting performance.\n",
    "    expects data_path upon instantiation, which is a directory in which\n",
    "    the 3000000x300 pretrained Google News vectors binary file should be at\n",
    "    very least, and will create embeddings and vocab (embed.dat, embed.vocab)\n",
    "    in that directory if they do not exist. In order to perform training, the\n",
    "    class expects 'dataset.csv' as well, which should have no header, and\n",
    "    three entries per datapoint (Error [1 for minor, 2 for major],\n",
    "    String 1 [first transcription],String 2 [second transcription]). Some\n",
    "    files will be created as a result of training (model.ckpt, fuzzy.csv).\n",
    "    \"\"\"\n",
    "    def __init__(self,data_path):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self._save_path = os.path.join(self.data_path,'model.ckpt')\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "        binary_file = os.path.join(self.data_path,\n",
    "                                   'GoogleNews-vectors-negative300.bin')\n",
    "        w2v_dat = os.path.join(self.data_path,'embed.dat')\n",
    "        w2v_vocab = os.path.join(self.data_path,'embed.vocab')\n",
    "\n",
    "        if not os.path.exists(w2v_dat):\n",
    "            print(\"Caching word embeddings in memmapped format.                     Please be patient...\")\n",
    "            wv = Word2Vec.load_word2vec_format(\n",
    "                binary_file,binary=True)\n",
    "            fp = np.memmap(w2v_dat, dtype=np.double,\n",
    "                           mode='w+', shape=wv.syn0.shape)\n",
    "            fp[:] = wv.syn0[:]\n",
    "            with open(w2v_vocab, \"w\") as f:\n",
    "                for _, w in sorted((voc.index, word)                                    for word, voc in wv.vocab.items()):\n",
    "                    print(w, file=f)\n",
    "            del fp, wv\n",
    "\n",
    "        # create word embeddings and mapping of vocabulary item to index\n",
    "        self.embeddings = np.memmap(w2v_dat, dtype=np.float64,\n",
    "                                    mode=\"r\", shape=(3000000, 300))\n",
    "        with open(w2v_vocab) as f:\n",
    "            vocab_list = map(lambda string: string.strip(), f.readlines())\n",
    "        self.vocab_dict = {w: i for i, w in enumerate(vocab_list)}\n",
    "\n",
    "        # mean of 20 rarest words, used as a stand-in for pairwise distances\n",
    "        # if a word is out-of-vocabulary\n",
    "        avg_rare_word = np.mean(np.vstack(self.embeddings[-20:]),axis=0)\n",
    "        self.bad_row = np.asarray([avg_rare_word])\n",
    "\n",
    "    def _index_check(self,features):\n",
    "        total = 0\n",
    "        for word in features:\n",
    "            if word in self.vocab_dict:\n",
    "                total+=self.vocab_dict[word]\n",
    "        return total\n",
    "\n",
    "    def _get_dist(self,s_1,s_2):\n",
    "        \"\"\"Return counts of in-vocabulary and out-of-vocabulary items per\n",
    "        string, means of embeddings per string, and Word Mover's Distance\n",
    "        between the two. Word embeddings and mappings were created upon\n",
    "        initialization of the class instance, and WMD with emd()\n",
    "        (Earth Mover's Distance) from PyEMD. Final shape is [1,612].\n",
    "        \"\"\"\n",
    "\n",
    "        #results_ = []\n",
    "        \n",
    "        ###############\n",
    "        #s_1 = re.sub(r\"\\bs\\d{0,2}\\b\",'speaker',s_1)\n",
    "        #s_1 = re.sub(r\"\\b\\d{5,6}\\b\",'timestamp',s_1)\n",
    "        #s_1 = re.sub(r'\\d+','digit',s_1)\n",
    "        s_1 = re.sub(r'-',' ',s_1)\n",
    "        s_1 = re.sub(r\"\\ba\\b\",'one',s_1)\n",
    "        \n",
    "        #s_2 = re.sub(r\"\\bs\\d{0,2}\\b\",'speaker',s_2)\n",
    "        #s_2 = re.sub(r\"\\b\\d{5,6}\\b\",'timestamp',s_2)\n",
    "        #s_2 = re.sub(r'\\d+','digit',s_2)\n",
    "        s_2 = re.sub(r'-',' ',s_2)\n",
    "        s_2 = re.sub(r\"\\ba\\b\",'one',s_2)\n",
    "        \n",
    "        \n",
    "        # moved this up here from mean of word embeddings section\n",
    "        s1_features = s_1.split()\n",
    "        s2_features = s_2.split()\n",
    "\n",
    "\n",
    "        S1_ = self.embeddings[[self.vocab_dict[w] for w in s1_features if w in self.vocab_dict]]\n",
    "        S2_ = self.embeddings[[self.vocab_dict[w] for w in s2_features if w in self.vocab_dict]]\n",
    "\n",
    "        S1 = np.asarray(S1_).reshape([-1,300])\n",
    "        S2 = np.asarray(S2_).reshape([-1,300])\n",
    "        \n",
    "        S1_zero_rows = 150 - S1.shape[0]\n",
    "        S2_zero_rows = 150 - S2.shape[0]\n",
    "        \n",
    "        \n",
    "        result = np.vstack((S1,\n",
    "                            np.zeros([S1_zero_rows,300]),\n",
    "                            S2,\n",
    "                            np.zeros([S2_zero_rows,300])))\n",
    "        \n",
    "        #results_.extend(S1_)\n",
    "        #results_.extend(S2_)\n",
    "        #results_.append(cosine(S1_,S2_))\n",
    "                \n",
    "        return result\n",
    "\n",
    "    def _data_generator(self,str_1,str_2):\n",
    "        \"\"\"\n",
    "        Transform two strings into a vector of 612 features as expected by the\n",
    "        TensorFlow model.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self._get_dist(str_1,str_2)\n",
    "\n",
    "        # X to be fed to the network\n",
    "        ########X = np.asarray(X).reshape((-1,609)) #612)) #12))#\"\"\"612))\"\"\"\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _train_data_generator(self,shuffle,seed):\n",
    "        \"\"\"\n",
    "        Transforms self.training_set ('dataset.csv' in self.data_path) and\n",
    "        self.fuzzy_path ('fuzzy.csv' in self.data_path) into useful features\n",
    "        to train the model, and transforms error type (1 for minor,\n",
    "        2 for major) into one-hot vector of length 2 (i.e., [1,0] for minor,\n",
    "        [0,1] for major). Data are shuffled by default, as they are sorted by\n",
    "        error type in the original training sets.\n",
    "\n",
    "        Returns: X, Y, shuffled indices, original X (as pairs of strings)\n",
    "            The latter two are included purely for examining performance.\n",
    "        \"\"\"\n",
    "\n",
    "        # original training set cols are Error_type, Str_1, Str_2\n",
    "        warnings.warn(\"still using training_set instead of self.training_set\")\n",
    "        X_in = np.genfromtxt(training_set,\n",
    "                      delimiter=',',usecols=(1,2),dtype=str)\n",
    "        Y_in = np.genfromtxt(training_set,\n",
    "                      delimiter=',',usecols=(0)).reshape((-1,1))\n",
    "\n",
    "        #X = []\n",
    "        X = np.empty([300,300])\n",
    "        Y = []\n",
    "        bad = []\n",
    "        \n",
    "        for i,strings in enumerate(X_in):\n",
    "            try:\n",
    "                scores = self._get_dist(strings[0],strings[1])\n",
    "                #X.extend(scores)\n",
    "                X = np.dstack((X,scores))\n",
    "                \n",
    "                # target\n",
    "                Y.append(Y_in[i])\n",
    "            except:\n",
    "                bad.append(i)\n",
    "                continue\n",
    "        self.bad = bad\n",
    "        \n",
    "        X = np.asarray(X).reshape((-1,300,300))#612)) #12))#\"\"\"612))\"\"\"\n",
    "        Y = np.asarray(Y).reshape((-1,1))\n",
    "\n",
    "        # unshuffled indices\n",
    "        indices = range(X.shape[0])\n",
    "\n",
    "        # randomly shuffle the data\n",
    "        if shuffle:\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "\n",
    "        # transform Y from either 1 or 2 to a one-hot vector ([1,0] or [0,1])\n",
    "        y_list = []\n",
    "        for i, label in enumerate(Y):\n",
    "            if label == 2:\n",
    "                label = 1\n",
    "                y_list.append(np.insert(label,0,0))\n",
    "            elif label == 1:\n",
    "                y_list.append(np.insert(label,1,0))\n",
    "            else:\n",
    "                raise ValueError(\"Y label must be either 1 (minor) or                                     2 (major). Problem at index \", indices[i])\n",
    "        Y = np.asarray(y_list)\n",
    "\n",
    "        return X,Y,indices,X_in\n",
    "\n",
    "    def _batch_norm_wrapper(self,inputs,training,decay=0.999):\n",
    "\n",
    "        scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "        beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "        pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]),\n",
    "                               trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]),\n",
    "                              trainable=False)\n",
    "\n",
    "        if training:\n",
    "            batch_mean,batch_var = tf.nn.moments(inputs,[0])\n",
    "            train_mean = pop_mean.assign(pop_mean*decay+batch_mean*(1-decay))\n",
    "            train_var = pop_var.assign(pop_var*decay+batch_var*(1-decay))\n",
    "            with tf.control_dependencies([train_mean,train_var]):\n",
    "                return tf.nn.batch_normalization(inputs,\n",
    "                                batch_mean,batch_var,beta,scale,self.epsilon)\n",
    "        else:\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                            pop_mean,pop_var,beta,scale,self.epsilon)\n",
    "\n",
    "    def _build_graph(self,training):\n",
    "\n",
    "        # inputs and outputs (latter are one-hot vectors)\n",
    "        X = tf.placeholder(tf.float32, shape=[None,300,300])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None,2])\n",
    "        lr = tf.placeholder(tf.float32)\n",
    "        glob_step = tf.Variable(0,dtype=tf.float32,trainable=False)\n",
    "\n",
    "        weight_shape1 = [300,300,256]\n",
    "        weight_shape2 = [256,128]\n",
    "        weight_shape3 = [128,16]\n",
    "        weight_shape4 = [16,2]\n",
    "\n",
    "        [n_inputs1,n_outputs1,n_inputs3,n_outputs3,n_outputs_final] =             weight_shape1[0],weight_shape1[1],weight_shape3[0],             weight_shape3[1],weight_shape4[1]\n",
    "\n",
    "        init_range1 = tf.sqrt(6.0/(n_inputs1+n_outputs1))\n",
    "        init_range2 = tf.sqrt(6.0/(n_outputs1+n_inputs3))\n",
    "        init_range3 = tf.sqrt(6.0/(n_inputs3+n_outputs3))\n",
    "        init_range4 = tf.sqrt(6.0/(n_outputs3+n_outputs_final))\n",
    "        w1 = tf.Variable(tf.random_uniform(weight_shape1,\n",
    "                                           -init_range1,init_range1),name='w1')\n",
    "        w2 = tf.Variable(tf.random_uniform(weight_shape2,\n",
    "                                           -init_range2,init_range2),name='w2')\n",
    "        w3 = tf.Variable(tf.random_uniform(weight_shape3,\n",
    "                                           -init_range3,init_range3),name='w3')\n",
    "        w4 = tf.Variable(tf.random_uniform(weight_shape4,\n",
    "                                           -init_range4,init_range4),name='w4')\n",
    "        b = tf.Variable(tf.constant(.1,shape=[n_outputs_final]))\n",
    "\n",
    "\n",
    "        # network - batch normalization in training, relu activations\n",
    "        dot1 = tf.matmul(X,w1)\n",
    "        batch_normed1 = self._batch_norm_wrapper(dot1,training)\n",
    "        rel1 = tf.nn.relu(batch_normed1)\n",
    "\n",
    "        dot2 = tf.matmul(rel1,w2)\n",
    "        batch_normed2 = self._batch_norm_wrapper(dot2,training)\n",
    "        rel2 = tf.nn.relu(batch_normed2)\n",
    "\n",
    "        dot3 = tf.matmul(rel2,w3)\n",
    "        batch_normed3 = self._batch_norm_wrapper(dot3,training)\n",
    "        rel3 = tf.nn.relu(batch_normed3)\n",
    "\n",
    "        # softmax layer\n",
    "        logits = tf.matmul(rel3,w4)+b\n",
    "        probs_x = tf.nn.softmax(logits)\n",
    "\n",
    "        # cost:\n",
    "        #    per pair\n",
    "        rows_of_cost =             tf.nn.softmax_cross_entropy_with_logits(logits,Y,\n",
    "                                                    name='rows_of_cost')\n",
    "        #    average over all pairs\n",
    "        cost = tf.reduce_mean(rows_of_cost,reduction_indices=None,\n",
    "                              keep_dims=False,name='cost')\n",
    "\n",
    "        # gradients and training\n",
    "        opt = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "        train_op = opt.minimize(cost,global_step=glob_step,\n",
    "                                var_list=[w1,w2,w3,w4,b])\n",
    "\n",
    "        # predictions and accuracy\n",
    "        correct_prediction = tf.equal(tf.arg_max(probs_x,1),tf.arg_max(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "        return (X,Y),cost,train_op,accuracy,probs_x,lr,tf.train.Saver()\n",
    "\n",
    "    def train(self,shuffle=True,seed=42,validation_size=.2,test_size=.05):\n",
    "\n",
    "        \"\"\"\n",
    "        Train the model on the data stored in 'dataset.csv' in self.data_path.\n",
    "        This will check for a file named 'fuzzy.csv' first, which is the output\n",
    "        of self._get_fuzzy(), and creates it if it is not present.\n",
    "\n",
    "        Expected format of 'dataset.csv':\n",
    "            no header, three entries per row of (Error,String 1,String 2).\n",
    "            Error is an integer (1 for minor, 2 for major)\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_set = os.path.join(self.data_path,'dataset.csv')\n",
    "\n",
    "        # check if data has already been split before generating and splitting\n",
    "        try:\n",
    "            assert len(self.x_train)==len(self.raw_X)-int(len(self.raw_X)*(validation_size+test_size))\n",
    "        except (AssertionError,AttributeError):\n",
    "            print(\"Generating and splitting data...\")\n",
    "            X_data,Y_data,self.shuffled_idx,self.raw_X = self._train_data_generator(shuffle,seed)\n",
    "            \n",
    "            # create split indices for validation, test, and train sets\n",
    "            self._validation_test_split_idx = int(len(Y_data)*validation_size)\n",
    "            self._train_test_split_idx = int(len(Y_data)*test_size)+self._validation_test_split_idx\n",
    "\n",
    "            # split data\n",
    "            self.x_validation = X_data[:self._validation_test_split_idx]\n",
    "            self.x_test = X_data[self._validation_test_split_idx:\n",
    "                                 self._train_test_split_idx]\n",
    "            self.x_train = X_data[self._train_test_split_idx:]\n",
    "            self.y_validation = Y_data[:self._validation_test_split_idx]\n",
    "            self.y_test = Y_data[self._validation_test_split_idx:\n",
    "                                 self._train_test_split_idx]\n",
    "            self.y_train = Y_data[self._train_test_split_idx:]\n",
    "                \n",
    "        print(\"Training model...\")\n",
    "        # build and run network in training mode\n",
    "        tf.reset_default_graph()\n",
    "        (X,Y),cost,train_op,accuracy,probs_x,lr,saver = self._build_graph(training=True)\n",
    "        \n",
    "        # just putting 0 in so first accuracy can be compared to something for now\n",
    "        self.accuracy = [0]\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            mini_batch_size = 32\n",
    "            start_end = zip(range(0,len(self.x_train),mini_batch_size),\n",
    "                           range(mini_batch_size,len(self.x_train)+1,\n",
    "                                 mini_batch_size))\n",
    "            cost_list = []\n",
    "\n",
    "            # number of training epochs\n",
    "            num_passes = 31\n",
    "            for pass_i in range(num_passes):\n",
    "                for (s,e) in start_end:\n",
    "\n",
    "                    # learning rate scheduling\n",
    "                    #if pass_i < 20:\n",
    "                    cost_list.append(sess.run(\n",
    "                            [cost],feed_dict={X:self.x_train[s:e,],\n",
    "                                              Y:self.y_train[s:e],\n",
    "                                              lr:.09}))\n",
    "                    sess.run([train_op],feed_dict={X:self.x_train[s:e,],\n",
    "                                                   Y:self.y_train[s:e],\n",
    "                                                   lr:.09})\n",
    "                    #else:\n",
    "                    #    cost_list.append(sess.run(\n",
    "                    #            [cost],feed_dict={X:self.x_train[s:e,],\n",
    "                    #                              Y:self.y_train[s:e],\n",
    "                    #                              lr:.05}))\n",
    "                    #    sess.run([train_op],feed_dict={X:self.x_train[s:e,],\n",
    "                    #                                   Y:self.y_train[s:e],\n",
    "                    #                                   lr:.05})\n",
    "                # show current accuracy\n",
    "                if pass_i % 5 == 0:\n",
    "                    result = sess.run([accuracy],\n",
    "                                      feed_dict={X:self.x_validation,\n",
    "                                                 Y:self.y_validation})\n",
    "                    print('Pass number: ',pass_i,\n",
    "                          ' -- validation set accuracy: ',result[0])\n",
    "                    \n",
    "                    # save model in self._save_path if accuracy is better than any previous run\n",
    "                    if result[0] > max(self.accuracy):\n",
    "                        save_path = saver.save(sess,self._save_path)\n",
    "                    self.accuracy.append(result[0])\n",
    "\n",
    "                    \n",
    "            # save cost and result lists for examining model performance\n",
    "            self._cost_list = cost_list\n",
    "            self._result_list = sess.run([tf.arg_max(probs_x,1)],\n",
    "                                         feed_dict={X:self.x_test,\n",
    "                                                    Y:self.y_test})\n",
    "            print(\"Model saved in file: {}\".format(save_path))\n",
    "\n",
    "    def check_results(self):\n",
    "        \"\"\"\n",
    "        Prints a confusion matrix of performance on the test set,\n",
    "        and instantiates lists of True Positive, True Negative,\n",
    "        False Positive, and False Negative for inspection as\n",
    "        self._TP, self._TN, self._FP, self._FN.\n",
    "        \"\"\"\n",
    "\n",
    "        # print confusion matrix\n",
    "        true_y_labels = np.array(self.y_test[:,1])\n",
    "        print('\\t\\tPredicted:')\n",
    "        print('\\t\\tmin. maj.')\n",
    "        print('Actual:\\t min.',\n",
    "              confusion_matrix(true_y_labels,self._result_list[0])[0])\n",
    "        print('    \\t maj.',\n",
    "              confusion_matrix(true_y_labels,self._result_list[0])[1])\n",
    "\n",
    "        # identify predicted and true positives and negatives\n",
    "        predicted_pos = np.where(self._result_list[0]==1)\n",
    "        predicted_neg = np.where(self._result_list[0]==0)\n",
    "        actual_pos = np.where(np.argmax(self.y_test,1)==1)\n",
    "        actual_neg = np.where(np.argmax(self.y_test,1)==0)\n",
    "\n",
    "        # indices of shuffled and split data (just y_test)\n",
    "        true_pos = np.intersect1d(predicted_pos,actual_pos).tolist()\n",
    "        true_neg = np.intersect1d(predicted_neg,actual_neg).tolist()\n",
    "        false_pos = np.intersect1d(predicted_pos,actual_neg).tolist()\n",
    "        false_neg = np.intersect1d(predicted_neg,actual_pos).tolist()\n",
    "        y_indices = self.shuffled_idx[self._validation_test_split_idx:\n",
    "                                      self._train_test_split_idx]\n",
    "\n",
    "        # create lists of true and false positives and negatives\n",
    "        self._TP = [list(self.raw_X[y_indices[i]]) for i in true_pos]\n",
    "        self._TN = [list(self.raw_X[y_indices[i]]) for i in true_neg]\n",
    "        self._FP = [list(self.raw_X[y_indices[i]]) for i in false_pos]\n",
    "        self._FN = [list(self.raw_X[y_indices[i]]) for i in false_neg]\n",
    "\n",
    "    def predict(self,csv_file):\n",
    "        \"\"\"\n",
    "        Predicts the type of error between the two strings in each row of\n",
    "        a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            0 for minor, 1 for major,\n",
    "            'No error' for identical strings,\n",
    "            and 'Unknown' if a prediction cannot be made (could change to 0).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        # build graph and initialize session\n",
    "        tf.reset_default_graph()\n",
    "        (X,_),_,_,_,pred_y,lr,saver = self._build_graph(training=False)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver.restore(sess,self._save_path)\n",
    "\n",
    "            # generate calculations from 2d array of input strings\n",
    "            for row in np.genfromtxt(csv_file,dtype='str',delimiter=','):\n",
    "                str_1,str_2 = row[0],row[1]\n",
    "\n",
    "                # strings identical\n",
    "                if str_1 == str_2:\n",
    "                    predictions.append('No error')\n",
    "                    continue\n",
    "\n",
    "                # model prediction\n",
    "                try:\n",
    "                    pred = sess.run([tf.arg_max(pred_y,1)],\n",
    "                                    feed_dict=\\\n",
    "                                    {X: self._data_generator(str_1,str_2)})\n",
    "                    predictions.append(str(pred[0][0]+1))\n",
    "\n",
    "                # can't predict\n",
    "                except:\n",
    "                    predictions.append('Unknown')\n",
    "        \n",
    "        print(','.join(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/users/rutherford/desktop/data'\n",
    "checker = error_checker(path)\n",
    "training_set = os.path.join(path,'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.genfromtxt(training_set,delimiter=',',usecols=(1,2),dtype=str)\n",
    "first = checker._get_dist(x[0][0],x[0][1])\n",
    "second = checker._get_dist(x[1][0],x[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06738281, -0.12451172,  0.1796875 , ...,  0.12988281,\n",
       "        -0.43554688,  0.23046875],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dstack((np.empty([300,300]),first,second))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:148: UserWarning: still using training_set instead of self.training_set\n"
     ]
    }
   ],
   "source": [
    "#runs forever.. oom?\n",
    "x,y,idx,raw = checker._train_data_generator(True,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prd = np.asarray([1,2,1,2,1,1,1,1,2,1,2,1,1,1,2,1,2,1,2,1,2,1])\n",
    "tru = np.asarray([2,2,1,2,1,2,1,1,2,1,2,1,2,1,1,2,1,2,1,2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  5, 12, 15, 17, 19]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((prd == 1) & (tru == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
