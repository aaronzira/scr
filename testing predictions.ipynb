{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean these up\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import re\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from pyemd import emd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/20154303/pandas-read-csv-expects-wrong-number-of-columns-with-ragged-csv-file\n",
    "\n",
    "def get_fuzzy(data_path):\n",
    "    \"\"\"\n",
    "    Compute fuzzy wuzzy calculations on each pair of strings\n",
    "    Return: the resulting dataframe as fuzzy.csv\n",
    "    \"\"\"\n",
    "    temp = pd.read_csv(data_path,sep='^',header=None,prefix='X')\n",
    "    temp2 = temp.X0.str.split(',',expand=True)\n",
    "\n",
    "    df = pd.DataFrame(columns=['ratio','partial','sort','set'],\n",
    "                      index=range(len(temp2)))\n",
    "\n",
    "    for row in range(temp2.shape[0]):\n",
    "        df['ratio'][row] = fuzz.ratio(temp2[1][row],temp2[2][row])\n",
    "        df['partial'][row] = fuzz.partial_ratio(temp2[1][row],temp2[2][row])\n",
    "        df['sort'][row] = fuzz.token_sort_ratio(temp2[1][row],temp2[2][row])\n",
    "        df['set'][row] = fuzz.token_set_ratio(temp2[1][row],temp2[2][row])\n",
    "\n",
    "    df.to_csv('{}/fuzzy.csv'.format(os.path.dirname(data_path)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://vene.ro/blog/word-movers-distance-in-python.html\n",
    "\n",
    "def get_dist(s_1,s_2,word_embeds,vocab_d):\n",
    "    \"\"\"compare strings s_1 and s_2, using word embeddings from word_embeds, \n",
    "    and index mappings (word:index in embeddings) in dictionary vocab_d\"\"\"\n",
    "    \n",
    "    # eliminate underscores -- necessary?\n",
    "    ##s_1 = re.sub(r'\\_+','',s_1)\n",
    "    ##s_2 = re.sub(r'\\_+','',s_2)\n",
    "    \n",
    "    # fit cv on the strings\n",
    "    # no more stop words\n",
    "    vect = CountVectorizer().fit([s_1, s_2])\n",
    "    \n",
    "    # get normalized 'flow' vectors\n",
    "    v_1, v_2 = vect.transform([s_1, s_2])\n",
    "    v_1 = v_1.toarray().ravel().astype(np.float64)\n",
    "    v_2 = v_2.toarray().ravel().astype(np.float64)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    \n",
    "    # get normalized distance matrix for words in both docs\n",
    "    W_ = word_embeds[[vocab_d[w] for w in vect.get_feature_names()]]\n",
    "    D_ = euclidean_distances(W_).astype(np.float64)\n",
    "    D_ /= D_.max()\n",
    "\n",
    "    distances = emd(v_1,v_2,D_)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data_path,word_embeds,index_dict,fuzzy_path,\n",
    "                   distance_func=get_dist,shuffle=True,seed=42):\n",
    "    \"\"\"shuffle is default true because data are sorted.\n",
    "    \n",
    "    Returns: X, Y, shuffled indices, and X (as pairs of strings)\n",
    "    \"\"\"\n",
    "    X_in = np.genfromtxt(data_path,\n",
    "                  delimiter=',',usecols=(1,2),dtype=str)\n",
    "    Y_in = np.genfromtxt(data_path,\n",
    "                  delimiter=',',usecols=(0)).reshape((-1,1))\n",
    "    # fuzzy_file columns are: simple ratio, partial ratio, \n",
    "    # token sort ratio, and token set ratio\n",
    "    fuzzy_file = np.genfromtxt(fuzzy_path,\n",
    "              delimiter=',',dtype=float,skip_header=1)\n",
    "    # string lengths for each pair\n",
    "    str1_len = [len(pair[0]) for pair in X_in]\n",
    "    str2_len = [len(pair[1]) for pair in X_in]\n",
    "\n",
    "    # exclude any observations where the WMD produces a nan, either\n",
    "    # because of a division by zero or only stopword strings <<but now stopwords gone>>\n",
    "    # raises a couple warnings about divide\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # could put in some check here to say if total words in both is less than 3,\n",
    "    # put in some fake super rare words in the cv\n",
    "    \n",
    "    for i, strings in enumerate(X_in):\n",
    "        try:\n",
    "            score = distance_func(strings[0],strings[1],word_embeds,index_dict)\n",
    "            if score >= 0: #use a cleaner way to check if it's a number\n",
    "                X.append(score)\n",
    "                # additional features from fuzzywuzzy\n",
    "                X.append(fuzzy_file[i][0])\n",
    "                X.append(fuzzy_file[i][1])\n",
    "                X.append(fuzzy_file[i][2])\n",
    "                X.append(fuzzy_file[i][3])\n",
    "                # string lengths as features\n",
    "                X.append(str1_len[i])\n",
    "                X.append(str2_len[i])\n",
    "                \n",
    "                Y.append(Y_in[i])\n",
    "        #save the words that cause exceptions to a list?\n",
    "        except ValueError:\n",
    "            continue\n",
    "        except KeyError:\n",
    "            continue\n",
    "    X = np.asarray(X).reshape((-1,7))#3\n",
    "    Y = np.asarray(Y).reshape((-1,1))\n",
    "    \n",
    "    indices = range(X.shape[0])\n",
    "    # randomly shuffle the data\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        Y = Y[indices]\n",
    "    \n",
    "    # transform Y from either 1 or 2 to a one-hot vector \n",
    "    # indicating the index: 0 for minor, 1 for major\n",
    "    # could also make this an optional parameter\n",
    "    y_list = []\n",
    "    for i, label in enumerate(Y):\n",
    "        if label == 2:\n",
    "            label = 1\n",
    "            y_list.append(np.insert(label,0,0))\n",
    "        elif label == 1:\n",
    "            y_list.append(np.insert(label,1,0))\n",
    "        else:\n",
    "            raise ValueError(\"Y label must be either 1 (minor) or 2 (major). \\\n",
    "                             Problem at index \", i)\n",
    "    Y = np.asarray(y_list)\n",
    "    \n",
    "    return X,Y,indices,X_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_file = \"/Users/Rutherford/Desktop/data/GoogleNews-vectors-negative300.bin\"\n",
    "w2v_dat = \"/Users/Rutherford/Desktop/data/embed.dat\"\n",
    "w2v_vocab = \"/Users/Rutherford/Desktop/data/embed.vocab\"\n",
    "vocab_size = 3000000\n",
    "embedding_dim = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.memmap(w2v_dat, dtype=np.float64, mode=\"r\", shape=(vocab_size, embedding_dim))\n",
    "with open(w2v_vocab) as f:\n",
    "    vocab_list = map(lambda string: string.strip(), f.readlines()) \n",
    "vocab_dict = {w: i for i, w in enumerate(vocab_list)}\n",
    "assert len(embeddings) == vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_fuzzy('/users/Rutherford/desktop/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y,ind,raw = data_generator('/users/Rutherford/desktop/test.csv',embeddings,vocab_dict,'/users/Rutherford/desktop/fuzzy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "g = tf.Graph() \n",
    "with g.as_default():\n",
    "    \n",
    "    # 7 inputs (WMD, 4 FuzzyWuzzy calculations, length of each string)\n",
    "    # 2 outputs (one-hot vector of index of prediction)\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 7])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    \n",
    "    glob_step = tf.Variable(0,dtype=tf.float32,trainable=False)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #lr = tf.train.exponential_decay(learning_rate=.1, \n",
    "    #                               global_step=glob_step, \n",
    "    #                               decay_steps=100, \n",
    "    #                               decay_rate=0.96, \n",
    "    #                               staircase=True)\n",
    "    \n",
    "    \n",
    "    weight_shape1 = [7,64]\n",
    "    weight_shape2 = [64,128]\n",
    "    weight_shape3 = [128,16]\n",
    "    weight_shape4 = [16,2]\n",
    "    \n",
    "\n",
    "    [n_inputs1,n_outputs1,n_inputs3,n_outputs3,n_outputs_final] = \\\n",
    "        weight_shape1[0],weight_shape1[1],weight_shape3[0], \\\n",
    "        weight_shape3[1],weight_shape4[1]\n",
    "    \n",
    "    # Weights initialized a la Glorot & Bengio paper\n",
    "    # but with batch normalization this may be irrelevant\n",
    "    init_range1 = tf.sqrt(6.0/(n_inputs1+n_outputs1))\n",
    "    init_range2 = tf.sqrt(6.0/(n_outputs1+n_inputs3))\n",
    "    init_range3 = tf.sqrt(6.0/(n_inputs3+n_outputs3))\n",
    "    init_range4 = tf.sqrt(6.0/(n_outputs3+n_outputs_final))\n",
    "    w1 = tf.Variable(tf.random_uniform(weight_shape1,\n",
    "                                       -init_range1,init_range1),name='w1')\n",
    "    w2 = tf.Variable(tf.random_uniform(weight_shape2,\n",
    "                                       -init_range2,init_range2),name='w2')\n",
    "    w3 = tf.Variable(tf.random_uniform(weight_shape3,\n",
    "                                       -init_range3,init_range3),name='w3')\n",
    "    w4 = tf.Variable(tf.random_uniform(weight_shape4,\n",
    "                                       -init_range4,init_range4),name='w4')\n",
    "    # no need for biases after batch normalizing\n",
    "    #b1 = tf.Variable(tf.constant(.1,shape=[n_outputs1]))\n",
    "    #b2 = tf.Variable(tf.constant(.1,shape=[n_inputs3]))\n",
    "    #b3 = tf.Variable(tf.constant(.1,shape=[n_outputs3]))\n",
    "    b = tf.Variable(tf.constant(.1,shape=[n_outputs_final]))\n",
    "        \n",
    "    # Network -- 3 batch normalized dropout layers\n",
    "    batch_normed1 = tf.contrib.layers.batch_norm(tf.matmul(X,w1)) \n",
    "    rel1 = tf.nn.relu(batch_normed1)\n",
    "    rel1_drop = tf.nn.dropout(rel1,keep_prob)\n",
    "    \n",
    "    batch_normed2 = tf.contrib.layers.batch_norm(tf.matmul(rel1_drop,w2)) \n",
    "    rel2 = tf.nn.relu(batch_normed2)\n",
    "    rel2_drop = tf.nn.dropout(rel2,keep_prob)\n",
    "    \n",
    "    batch_normed3 = tf.contrib.layers.batch_norm(tf.matmul(rel2_drop,w3)) \n",
    "    rel3 = tf.nn.relu(batch_normed3)\n",
    "    rel3_drop = tf.nn.dropout(rel3,keep_prob)\n",
    "    \n",
    "    logits = tf.matmul(rel3_drop,w4)+b\n",
    "    \n",
    "    # Predictions\n",
    "    probs_x = tf.nn.softmax(logits)\n",
    "    y_pred = tf.argmax(probs_x,dimension=1)\n",
    "    \n",
    "    # Cost\n",
    "    # per pair\n",
    "    rows_of_cost = \\\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits,Y,name='rows_of_cost')\n",
    "    # average over all pairs; loss\n",
    "    cost = tf.reduce_mean(rows_of_cost,reduction_indices=None,\n",
    "                          keep_dims=False,name='cost')\n",
    "\n",
    "    # gradients and training\n",
    "    opt = tf.train.AdagradOptimizer(learning_rate=.02)\n",
    "    train_op = opt.minimize(cost,global_step=glob_step,var_list=[w1,w2,w3,w4,b])\n",
    "    \n",
    "    # save model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    saver.restore(sess, \"/Users/Rutherford/Desktop/scribie data/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    print(sess.run(y_pred,feed_dict={X:x,keep_prob:1.}))\n",
    "    \"\"\"mini_batch_size = 32\n",
    "    start_end = zip(range(0,len(x_train),mini_batch_size), \n",
    "                   range(mini_batch_size,len(x_train)+1,mini_batch_size))\n",
    "    cost_list = []\n",
    "    num_passes = 401\n",
    "    for pass_i in range(num_passes):\n",
    "        for (s,e) in start_end:\n",
    "            cost_val,_ = sess.run([cost,train_op], #need a backslash here?\n",
    "                feed_dict={X: x_train[s:e,],Y: y_train[s:e],keep_prob:.8})\n",
    "            cost_list.append(cost_val)\n",
    "        if pass_i % 50 == 0: \n",
    "            test_result = sess.run([y_pred],feed_dict={X:x_test,keep_prob:1.})\n",
    "            # OOS accuracy\n",
    "            print(pass_i,np.mean(np.argmax(y_test,axis=1) == test_result[0]))\n",
    "    save_path = saver.save(sess,'{}/model.ckpt'.format(os.path.dirname(data_path)))\n",
    "    print(\"Model saved in file: {}\".format(save_path))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(s_1,s_2):\n",
    "    \"\"\"Return Word Mover's Distance between strings s_1 and s_2, \n",
    "    using word embeddings and mappings created upon initialization \n",
    "    of the class instance, and ultimately calculating the distance with\n",
    "    emd (Earth Mover's Distance) from PyEMD. \n",
    "    \"\"\"\n",
    "    cv_types = [\"\",\"lowercase=False\",\"stop_words='english'\",\n",
    "                \"stop_words='english',lowercase=False\"]\n",
    "    # fit CV on the strings, with no stop words\n",
    "    results_ = []\n",
    "    for method in cv_types:\n",
    "        try:\n",
    "            vect = CountVectorizer('{}'.format(method)).fit([s_1, s_2])        \n",
    "            # for getting rid of items not in Google vectors\n",
    "            features = np.asarray(vect.get_feature_names())\n",
    "            bad_indices = [idx for (idx,word) in \\\n",
    "                           enumerate(features) if word not in vocab_dict]\n",
    "\n",
    "            # get 'flow' vectors\n",
    "            v_1, v_2 = vect.transform([s_1, s_2])\n",
    "            v_1 = v_1.toarray().ravel().astype(np.float64)\n",
    "            v_2 = v_2.toarray().ravel().astype(np.float64)\n",
    "\n",
    "            # eliminate OOV items from vectors\n",
    "            features = np.delete(features,bad_indices)    \n",
    "            v_1 = np.delete(v_1,bad_indices)\n",
    "            v_2 = np.delete(v_2,bad_indices)\n",
    "\n",
    "            # normalize 'flow' vectors\n",
    "            ###v_1 /= v_1.sum()\n",
    "            ###v_2 /= v_2.sum()\n",
    "\n",
    "            # get normalized distance matrix for words in both strings\n",
    "            W_ = embeddings[[vocab_dict[w] for w in features]] ##  if w in self.vocab_dict\n",
    "            # print(W_) # empty list for just numbers \n",
    "            D_ = euclidean_distances(W_).astype(np.float64)\n",
    "            ###D_ /= D_.max()\n",
    "\n",
    "            # using emd (Earth Mover's Distance) from PyEMD\n",
    "            distances = emd(v_1,v_2,D_)\n",
    "            #print(distances)\n",
    "            results_.append(distances)\n",
    "        except ValueError:\n",
    "            return [0,0,0,0]#or like [-999...] but you'd have to fix logic check (>=0)\n",
    "\n",
    "    return results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'and'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-88f93f368847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"and\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'and'"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dist(\"i'm\",'i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dingy = np.asarray([['tiger','liger'],['man','ham'],['chinaman','heineman'],['beagle','eagle']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3650433100162997, 3.3650433100162997, 3.3650433100162997, 3.3650433100162997]\n",
      "[3.5730327376204007, 3.5730327376204007, 3.5730327376204007, 3.5730327376204007]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[4.521330026974679, 4.521330026974679, 4.521330026974679, 4.521330026974679]\n"
     ]
    }
   ],
   "source": [
    "for s1,s2 in dingy:\n",
    "    print(get_dist(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_1 = 'charles in charge'\n",
    "s_2 = 'of a life'\n",
    "vect = CountVectorizer().fit([s_1,s_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.asarray(vect.get_feature_names())\n",
    "bad_indices = [idx for (idx,word) in \\\n",
    "               enumerate(features) if word not in vocab_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_1, v_2 = vect.transform([s_1, s_2])\n",
    "v_1 = v_1.toarray().ravel().astype(np.float64)\n",
    "v_2 = v_2.toarray().ravel().astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(v_2[bad_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.delete(features,bad_indices)    \n",
    "v_1 = np.delete(v_1,bad_indices)\n",
    "v_2 = np.delete(v_2,bad_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  0.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_set = {'and','so','but','like','kinda','laughter','chuckle','pause'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(str_1,str_2):\n",
    "#def predict(self,pair_array)\n",
    "    \"\"\"\n",
    "    Predicts the type of error between the two strings.\n",
    "    Returns: \n",
    "        0 for minor, 1 for major, \n",
    "        'No error' for identical strings, \n",
    "        and 0 if a prediction cannot be made. # was 'Unknown'\n",
    "    \"\"\"\n",
    "    #for str_1,str_2 in pair_array:\n",
    "    #    same\n",
    "    if str_1 == str_2:\n",
    "        return 'No error'\n",
    "    if 0 < len(str_1.split()) < 4:\n",
    "        words = str_1.split()\n",
    "        if any([word in stop_words_set for word in words]):\n",
    "            return 0\n",
    "\n",
    "        try:                \n",
    "            maybe_timestamp = int(words[0])\n",
    "            # have to check the string, because if it starts with 000, those will be dropped\n",
    "            if 4 < len(words[0]) < 7:\n",
    "                return 0\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        return 'model prediction'\n",
    "    except:\n",
    "        #print(0)\n",
    "        return 0 #'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "0\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n",
      "model prediction\n"
     ]
    }
   ],
   "source": [
    "for pair in testing:\n",
    "    print(predict(pair[0],pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing = np.asarray([['tutoring', 'torturing'],\n",
    "       [\"i don't\", \"i'm trying to\"],\n",
    "       ['s', 's3'],\n",
    "       ['102302', '102315'],\n",
    "       ['bike bicycle', 'bike in that bicycle'],\n",
    "       ['____', 'planar'],\n",
    "       ['we are', \"that we're\"],\n",
    "       ['increased', 'increased 001514 s1'],\n",
    "       ['data', ''],\n",
    "       ['026142', '026152'],\n",
    "       ['expertise like', 'expertise'],\n",
    "       ['bits of', 'feeds twitter'],\n",
    "       ['idea', 'that they did'],\n",
    "       ['more', 'different'],\n",
    "       ['indoctrined yeah', 'indoctrine yeah'],\n",
    "       ['them 010021 s1 yeah 010034 s2', 'them'],\n",
    "       ['were taking', 'had taken'],\n",
    "       ['knew 000047 s ____ 000054 s', 'knew'],\n",
    "       ['money because', \"money 'cause\"],\n",
    "       ['front', 'front laughter 005572 s3'],\n",
    "       ['connatate', 'connotate'],\n",
    "       ['wba', 'wnba'],\n",
    "       [\"that's why\", 'so'],\n",
    "       ['flesh', 'fleshing'],\n",
    "       [\"s1 well you've\", \"s2 you've\"],\n",
    "       ['____', 'this would be'],\n",
    "       ['004195 s1', ''],\n",
    "       ['you', 'sure to'],\n",
    "       [\"we tell them don't run it\", \"something's gone wrong\"],\n",
    "       ['was like', 'was'],\n",
    "       ['side is', \"side it's\"],\n",
    "       ['came', \"came that's great\"],\n",
    "       ['guess', 'get'],\n",
    "       ['and so what', '09468 s what'],\n",
    "       ['flirt', 'flitter'],\n",
    "       ['pick', 'picked'],\n",
    "       ['____', 'our incumbents'],\n",
    "       ['tonight yeah', 'tonight'],\n",
    "       ['like', 'it'],\n",
    "       ['within', 'within the'],\n",
    "       ['____', 'proprietary'],\n",
    "       ['is photo creds so', 'is photo creds so that'],\n",
    "       ['____', '____ mike ____'],\n",
    "       ['sixty five hundred', '6500'],\n",
    "       ['input', 'inputted'],\n",
    "       ['integral differences which ____',\n",
    "        'individual differences if healthy'],\n",
    "       ['100000-dollar', '100000']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances###\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from pyemd import emd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_path = '/Users/Rutherford/Desktop/data'\n",
    "w2v_dat = os.path.join(data_path,'embed.dat') \n",
    "w2v_vocab = os.path.join(data_path,'embed.vocab')\n",
    "\n",
    "if not os.path.exists(w2v_dat):\n",
    "    print(\"Caching word embeddings in memmapped format. \\\n",
    "            Please be patient...\")\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    wv = Word2Vec.load_word2vec_format(\n",
    "        binary_file,binary=True)\n",
    "    fp = np.memmap(w2v_dat, dtype=np.double, \n",
    "                   mode='w+', shape=wv.syn0.shape)\n",
    "    fp[:] = wv.syn0[:]\n",
    "    with open(w2v_vocab, \"w\") as f:\n",
    "        for _, w in sorted((voc.index, word) \\\n",
    "                           for word, voc in wv.vocab.items()):\n",
    "            print(w, file=f)\n",
    "    del fp, wv\n",
    "\n",
    "# create word embeddings and mapping of vocabulary item to index\n",
    "embeddings = np.memmap(w2v_dat, dtype=np.float64, \n",
    "                            mode=\"r\", shape=(3000000, 300))\n",
    "with open(w2v_vocab) as f:\n",
    "    vocab_list = map(lambda string: string.strip(), f.readlines()) \n",
    "vocab_dict = {w: i for i, w in enumerate(vocab_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in = np.genfromtxt('/Users/Rutherford/Desktop/data/dataset.csv',\n",
    "              delimiter=',',usecols=(1,2),dtype=str)\n",
    "Y_in = np.genfromtxt('/Users/Rutherford/Desktop/data/dataset.csv',\n",
    "              delimiter=',',usecols=(0)).reshape((-1,1))\n",
    "fuzzy_file = np.genfromtxt('/Users/Rutherford/Desktop/data/fuzzy.csv',\n",
    "          delimiter=',',dtype=float,skip_header=1)\n",
    "\n",
    "\n",
    "str1_len = [len(pair[0]) for pair in X_in]\n",
    "str2_len = [len(pair[1]) for pair in X_in]\n",
    "\n",
    "str1_count = [len(pair[0].split()) for pair in X_in]\n",
    "str2_count = [len(pair[1].split()) for pair in X_in]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
