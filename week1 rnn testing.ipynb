{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embeds(data_path):\n",
    "\n",
    "    data_path = data_path\n",
    "    _save_path = os.path.join(data_path,'model.ckpt')\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    binary_file = os.path.join(data_path,\n",
    "                               'GoogleNews-vectors-negative300.bin')\n",
    "    w2v_dat = os.path.join(data_path,'embed.dat')\n",
    "    w2v_vocab = os.path.join(data_path,'embed.vocab')\n",
    "\n",
    "    if not os.path.exists(w2v_dat):\n",
    "        print(\"Caching word embeddings in memmapped format.\\nPlease be patient...\")\n",
    "        wv = Word2Vec.load_word2vec_format(\n",
    "            binary_file,binary=True)\n",
    "        fp = np.memmap(w2v_dat, dtype=np.double,\n",
    "                       mode='w+', shape=wv.syn0.shape)\n",
    "        fp[:] = wv.syn0[:]\n",
    "        with open(w2v_vocab, \"w\") as f:\n",
    "            for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "                print(w, file=f)\n",
    "        del fp, wv\n",
    "\n",
    "    # create word embeddings and mapping of vocabulary item to index\n",
    "    embeddings = np.memmap(w2v_dat, dtype=np.float64,\n",
    "                                mode=\"r\", shape=(3000000, 300))\n",
    "    with open(w2v_vocab) as f:\n",
    "        vocab_list = map(lambda string: string.strip(), f.readlines())\n",
    "    vocab_dict = {w: i for i, w in enumerate(vocab_list)}\n",
    "\n",
    "    # mean of 20 rarest words, used as a stand-in for pairwise distances\n",
    "    # if a word is out-of-vocabulary\n",
    "    avg_rare_word = np.mean(np.vstack((embeddings[-20:])),axis=0)\n",
    "    \n",
    "    return embeddings,vocab_list,vocab_dict,avg_rare_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed,v_list,vocab,rare = build_embeds('/Users/Rutherford/Desktop/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context_win(phrase, win):\n",
    "    '''\n",
    "    win : int corresponding to the size of the window\n",
    "    given a list of indexes composing a sentence\n",
    "\n",
    "    l : array containing the word indices\n",
    "\n",
    "    return : a list of list of indexes corresponding\n",
    "    to context windows surrounding each word in the sentence\n",
    "    '''\n",
    "    \n",
    "    # does it make sense to give it index of rarest word?\n",
    "    l = [vocab[w] if w in vocab else -1 for w in phrase.split()]\n",
    "    \n",
    "    # window length must be odd and positive\n",
    "    assert (win % 2) == 1\n",
    "    assert win > 0\n",
    "    #l = list(l)  no longer needed once you had the list of indices be the input\n",
    "    \n",
    "    # pad with window_size//2 -1's on each side\n",
    "    lpadded = win // 2 * [-1] + l + win // 2 * [-1]\n",
    "    out = [lpadded[i:(i + win)] for i in range(len(l))]\n",
    "\n",
    "    assert len(out) == len(l)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_string = \"this is a phrase that I want to break into parts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_context = context_win(test_string,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# could have the input be a tensor rank 3? does that lose the point?\n",
    "# (height= #words in sentence, width=size of context window, depth = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[ 0.04516602, -0.04516602, -0.00393677, ...,  0.07958984,\n",
       "         0.07226562,  0.01300049],\n",
       "       [ 0.04516602, -0.04516602, -0.00393677, ...,  0.07958984,\n",
       "         0.07226562,  0.01300049],\n",
       "       [ 0.109375  ,  0.140625  , -0.03173828, ...,  0.00765991,\n",
       "         0.12011719, -0.1796875 ],\n",
       "       [ 0.00704956, -0.07324219,  0.171875  , ...,  0.01123047,\n",
       "         0.1640625 ,  0.10693359],\n",
       "       [ 0.04516602, -0.04516602, -0.00393677, ...,  0.07958984,\n",
       "         0.07226562,  0.01300049]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[[i for i in test_context[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, -1, 28, 4, -1],\n",
       " [-1, 28, 4, -1, 7880],\n",
       " [28, 4, -1, 7880, 3],\n",
       " [4, -1, 7880, 3, 20],\n",
       " [-1, 7880, 3, 20, 189],\n",
       " [7880, 3, 20, 189, -1],\n",
       " [3, 20, 189, -1, 843],\n",
       " [20, 189, -1, 843, 69],\n",
       " [189, -1, 843, 69, 1286],\n",
       " [-1, 843, 69, 1286, -1],\n",
       " [843, 69, 1286, -1, -1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[ 0.04516602,  0.04516602,  0.109375  ,  0.00704956,  0.04516602]],\n",
       "\n",
       "       [[-0.04516602, -0.04516602,  0.140625  , -0.07324219, -0.04516602]],\n",
       "\n",
       "       [[-0.00393677, -0.00393677, -0.03173828,  0.171875  , -0.00393677]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.07958984,  0.07958984,  0.00765991,  0.01123047,  0.07958984]],\n",
       "\n",
       "       [[ 0.07226562,  0.07226562,  0.12011719,  0.1640625 ,  0.07226562]],\n",
       "\n",
       "       [[ 0.01300049,  0.01300049, -0.1796875 ,  0.10693359,  0.01300049]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300 stacks, 1 row (first word in sentence), 5 columns (len of context window)\n",
    "embed[[i for i in test_context[0]]].T.reshape((300,1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (300, 1, 55) doesn't seem too reasonable, easy fix to 300, # words, window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for e,row in enumerate(test_context):\n",
    "    dic[e] = embed[[i for i in row]].T.reshape((300,1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in dic.keys():\n",
    "    if word == 0:\n",
    "        x = dic[word]\n",
    "    else:\n",
    "        x = np.dstack([x,dic[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thinking that the order didn't really matter as long as it's all intact\n",
    "# (window size, 300, # words in sentence)\n",
    "# .T : (11, 300, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for e,row in enumerate(test_context):\n",
    "    dic[e] = embed[[i for i in row]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in dic.keys():\n",
    "    if word == 0:\n",
    "        x = dic[word]\n",
    "    else:\n",
    "        x = np.dstack([x,dic[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and a la http://deeplearning.net/tutorial/rnnslu.html \n",
    "# where they just stick embeddings for each word in the row into the same row\n",
    "# i.e., result is [# words in sentence,300*window size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(len(test_context)):\n",
    "    if row == 0:\n",
    "        tut = np.concatenate([embed[word] for word in test_context[row]])\n",
    "    else:\n",
    "        tut = np.vstack((tut,np.concatenate([embed[word] for word in test_context[row]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedder(cntxt_array):\n",
    "    for row in range(len(cntxt_array)):\n",
    "        if row == 0:\n",
    "            tut = np.concatenate([embed[word] for word in cntxt_array[row]])\n",
    "        else:\n",
    "            tut = np.vstack((tut,np.concatenate([embed[word] for word in cntxt_array[row]])))\n",
    "    return tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for i,sent in enumerate(sentences[:10000]):\n",
    "    dic[i] = embedder(context_win(' '.join(sent),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6008 -- AZ - Dot Product with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puncs = '[!@#$%^&*()_+=:;\"/,]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_string2 = \"this # (isn't) ==: anything / to $ awe$ome write % home & 'bout, but here goez nuttin'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_string2 = re.sub(puncs,'',test_string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " \"isn't\",\n",
       " 'anything',\n",
       " 'to',\n",
       " 'aweome',\n",
       " 'write',\n",
       " 'home',\n",
       " \"'bout\",\n",
       " 'but',\n",
       " 'here',\n",
       " 'goez',\n",
       " \"nuttin'\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([embed[vocab[word]] if word in vocab else embed[-1] for word in test_string2.split()]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = {1:2,2:1,3:1,4:2,'dog':1,500:1,51:0,50:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "51\n",
      "500\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "for k in dic.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Y = np.asarray([1,2,1,2,2,2,1,2,1,1,1,1,1,2,1,1,2]).reshape([-1,1])\n",
    "#np.asarray([[1,0] if target == 1 else [0,1] for target in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import euclidean_distances,confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from pyemd import emd\n",
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "class error_checker():\n",
    "    \"\"\"\n",
    "    Error checker class that builds embeddings upon instantiation, is capable\n",
    "    of being retrained, making predictions, and inspecting performance.\n",
    "    expects data_path upon instantiation, which is a directory in which\n",
    "    the 3000000x300 pretrained Google News vectors binary file should be at\n",
    "    very least, and will create embeddings and vocab (embed.dat, embed.vocab)\n",
    "    in that directory if they do not exist. In order to perform training, the\n",
    "    class expects 'dataset.csv' as well, which should have no header, and\n",
    "    three entries per datapoint (Error [1 for minor, 2 for major],\n",
    "    String 1 [first transcription],String 2 [second transcription]). Some\n",
    "    files will be created as a result of training (model.ckpt, fuzzy.csv).\n",
    "    \"\"\"\n",
    "    def __init__(self,data_path):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self._save_path = os.path.join(self.data_path,'model.ckpt')\n",
    "        self.epsilon = 1e-4\n",
    "        self.puncs = '[!@#$%^&*()_+=:;\"/,]+'\n",
    "        \n",
    "        self.training_set = os.path.join(self.data_path,'sentences.csv')\n",
    "\n",
    "        binary_file = os.path.join(self.data_path,\n",
    "                                   'GoogleNews-vectors-negative300.bin')\n",
    "        w2v_dat = os.path.join(self.data_path,'embed.dat')\n",
    "        w2v_vocab = os.path.join(self.data_path,'embed.vocab')\n",
    "\n",
    "        if not os.path.exists(w2v_dat):\n",
    "            print(\"Caching word embeddings in memmapped format.                     Please be patient...\")\n",
    "            wv = Word2Vec.load_word2vec_format(\n",
    "                binary_file,binary=True)\n",
    "            fp = np.memmap(w2v_dat, dtype=np.double,\n",
    "                           mode='w+', shape=wv.syn0.shape)\n",
    "            fp[:] = wv.syn0[:]\n",
    "            with open(w2v_vocab, \"w\") as f:\n",
    "                for _, w in sorted((voc.index, word)                                    for word, voc in wv.vocab.items()):\n",
    "                    print(w, file=f)\n",
    "            del fp, wv\n",
    "\n",
    "        # create word embeddings and mapping of vocabulary item to index\n",
    "        self.embeddings = np.memmap(w2v_dat, dtype=np.float64,\n",
    "                                    mode=\"r\", shape=(3000000, 300))\n",
    "        with open(w2v_vocab) as f:\n",
    "            vocab_list = map(lambda string: string.strip(), f.readlines())\n",
    "        self.vocab_dict = {w: i for i, w in enumerate(vocab_list)}\n",
    "\n",
    "        # mean of 20 rarest words, used as a stand-in for pairwise distances\n",
    "        # if a word is out-of-vocabulary\n",
    "        self.avg_rare_word = np.mean(np.vstack((self.embeddings[-20:])),axis=0)\n",
    "\n",
    "    def _data_generator(self,string):\n",
    "        \"\"\"\n",
    "        Transform string into array of embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        wordlist = re.sub(puncs,'',string).split()\n",
    "        X = np.asarray([self.embeddings[self.vocab_dict[word]] if word in self.vocab_dict \n",
    "                               else self.avg_rare_word\n",
    "                               for word in wordlist])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _train_data_generator(self,seed):\n",
    "        \"\"\"\n",
    "        Returns: X, Y, shuffled indices, original X (as pairs of strings)\n",
    "            The latter two are included purely for examining performance.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        X_in = np.genfromtxt(self.training_set,\n",
    "                      delimiter=',',usecols=(1),dtype=str)\n",
    "        Y_in = np.genfromtxt(self.training_set,\n",
    "                      delimiter=',',usecols=(0)).reshape((-1,1))\n",
    "\n",
    "        #X = {}\n",
    "        X = []\n",
    "        Y = []\n",
    "        indices = []\n",
    "        bad = []\n",
    "\n",
    "        for i,string in enumerate(X_in):\n",
    "            try:\n",
    "                \n",
    "                #X[i] = self._data_generator(string)\n",
    "                X.append(self._data_generator(string))\n",
    "\n",
    "                # target\n",
    "                Y.append(Y_in[i])\n",
    "                indices.append(i)\n",
    "            except:\n",
    "                bad.append(i)\n",
    "                continue\n",
    "        self.bad = bad\n",
    "        \n",
    "        \n",
    "        \"\"\"############\"\"\"\n",
    "        X = np.asarray(X)#.reshape((len(X),-1))\n",
    "        Y = np.asarray(Y).reshape((-1,1))\n",
    "\n",
    "        # randomly shuffle the data\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        #X = X[indices]\n",
    "        Y = Y[indices]\n",
    "\n",
    "        # transform Y from either 1 or 2 to a one-hot vector ([1,0] or [0,1])\n",
    "        Y = np.asarray([[1,0] if target == 1 else [0,1] for target in Y])\n",
    "        \n",
    "        # X_in is NOT shuffled for the purpose of testing later\n",
    "        return X,Y,indices,X_in\n",
    "\n",
    "    def _build_graph(self,training):\n",
    "\n",
    "        # inputs and outputs (latter are one-hot vectors)\n",
    "        X = tf.placeholder(tf.float32, shape=[None,12])#\"\"\"612])\"\"\"\n",
    "        Y = tf.placeholder(tf.float32, shape=[None,2])\n",
    "        lr = tf.placeholder(tf.float32)\n",
    "        glob_step = tf.Variable(0,dtype=tf.float32,trainable=False)\n",
    "\n",
    "        weight_shape1 = [12,256]#\"\"\"6\"\"\"\n",
    "        weight_shape2 = [256,128]\n",
    "        weight_shape3 = [128,16]\n",
    "        weight_shape4 = [16,2]\n",
    "\n",
    "        [n_inputs1,n_outputs1,n_inputs3,n_outputs3,n_outputs_final] =             weight_shape1[0],weight_shape1[1],weight_shape3[0],             weight_shape3[1],weight_shape4[1]\n",
    "\n",
    "        init_range1 = tf.sqrt(6.0/(n_inputs1+n_outputs1))\n",
    "        init_range2 = tf.sqrt(6.0/(n_outputs1+n_inputs3))\n",
    "        init_range3 = tf.sqrt(6.0/(n_inputs3+n_outputs3))\n",
    "        init_range4 = tf.sqrt(6.0/(n_outputs3+n_outputs_final))\n",
    "        w1 = tf.Variable(tf.random_uniform(weight_shape1,\n",
    "                                           -init_range1,init_range1),name='w1')\n",
    "        w2 = tf.Variable(tf.random_uniform(weight_shape2,\n",
    "                                           -init_range2,init_range2),name='w2')\n",
    "        w3 = tf.Variable(tf.random_uniform(weight_shape3,\n",
    "                                           -init_range3,init_range3),name='w3')\n",
    "        w4 = tf.Variable(tf.random_uniform(weight_shape4,\n",
    "                                           -init_range4,init_range4),name='w4')\n",
    "        b = tf.Variable(tf.constant(.1,shape=[n_outputs_final]))\n",
    "\n",
    "\n",
    "        # network - batch normalization in training, relu activations\n",
    "        dot1 = tf.matmul(X,w1)\n",
    "        batch_normed1 = self._batch_norm_wrapper(dot1,training)\n",
    "        rel1 = tf.nn.relu(batch_normed1)\n",
    "\n",
    "        dot2 = tf.matmul(rel1,w2)\n",
    "        batch_normed2 = self._batch_norm_wrapper(dot2,training)\n",
    "        rel2 = tf.nn.relu(batch_normed2)\n",
    "\n",
    "        dot3 = tf.matmul(rel2,w3)\n",
    "        batch_normed3 = self._batch_norm_wrapper(dot3,training)\n",
    "        rel3 = tf.nn.relu(batch_normed3)\n",
    "\n",
    "        # softmax layer\n",
    "        logits = tf.matmul(rel3,w4)+b\n",
    "        probs_x = tf.nn.softmax(logits)\n",
    "\n",
    "        # cost:\n",
    "        #    per pair\n",
    "        rows_of_cost =             tf.nn.softmax_cross_entropy_with_logits(logits,Y,\n",
    "                                                    name='rows_of_cost')\n",
    "        #    average over all pairs\n",
    "        cost = tf.reduce_mean(rows_of_cost,reduction_indices=None,\n",
    "                              keep_dims=False,name='cost')\n",
    "\n",
    "        # gradients and training\n",
    "        opt = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "        train_op = opt.minimize(cost,global_step=glob_step,\n",
    "                                var_list=[w1,w2,w3,w4,b])\n",
    "\n",
    "        # predictions and accuracy\n",
    "        correct_prediction = tf.equal(tf.arg_max(probs_x,1),tf.arg_max(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "        return (X,Y),cost,train_op,accuracy,probs_x,lr,tf.train.Saver()\n",
    "\n",
    "    def train(self,shuffle=True,seed=42,validation_size=.2,test_size=.1):\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Generating and splitting data...\")\n",
    "        X_data,Y_data,self.shuffled_idx,self.raw_X = self._train_data_generator(shuffle,seed)\n",
    "                \n",
    "        # create split indices for validation, test, and train sets\n",
    "        self._validation_test_split_idx = int(len(Y_data)*validation_size)\n",
    "        self._train_test_split_idx =                    int(len(Y_data)*test_size)+self._validation_test_split_idx\n",
    "\n",
    "        # split data\n",
    "        self.x_validation = X_data[:self._validation_test_split_idx]\n",
    "        self.x_test = X_data[self._validation_test_split_idx:\n",
    "                             self._train_test_split_idx]\n",
    "        self.x_train = X_data[self._train_test_split_idx:]\n",
    "        self.y_validation = Y_data[:self._validation_test_split_idx]\n",
    "        self.y_test = Y_data[self._validation_test_split_idx:\n",
    "                             self._train_test_split_idx]\n",
    "        self.y_train = Y_data[self._train_test_split_idx:]\n",
    "\n",
    "        print(\"Training model...\")\n",
    "        # build and run network in training mode\n",
    "        tf.reset_default_graph()\n",
    "        (X,Y),cost,train_op,accuracy,probs_x,lr,saver =                 self._build_graph(training=True)\n",
    "\n",
    "        self.accuracy = []\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            mini_batch_size = 64\n",
    "            start_end = zip(range(0,len(self.x_train),mini_batch_size),\n",
    "                           range(mini_batch_size,len(self.x_train)+1,\n",
    "                                 mini_batch_size))\n",
    "            cost_list = []\n",
    "\n",
    "            # number of training epochs\n",
    "            num_passes = 101\n",
    "            for pass_i in range(num_passes):\n",
    "                for (s,e) in start_end:\n",
    "\n",
    "                    # learning rate scheduling\n",
    "                    if pass_i < 20:\n",
    "                        cost_list.append(sess.run(\n",
    "                                [cost],feed_dict={X:self.x_train[s:e,],\n",
    "                                                  Y:self.y_train[s:e],\n",
    "                                                  lr:.9}))\n",
    "                        sess.run([train_op],feed_dict={X:self.x_train[s:e,],\n",
    "                                                       Y:self.y_train[s:e],\n",
    "                                                       lr:.9})\n",
    "                    else:\n",
    "                        cost_list.append(sess.run(\n",
    "                                [cost],feed_dict={X:self.x_train[s:e,],\n",
    "                                                  Y:self.y_train[s:e],\n",
    "                                                  lr:.0005}))\n",
    "                        sess.run([train_op],feed_dict={X:self.x_train[s:e,],\n",
    "                                                       Y:self.y_train[s:e],\n",
    "                                                       lr:.0005})\n",
    "                # show current accuracy\n",
    "                if pass_i % 5 == 0:\n",
    "                    result = sess.run([accuracy],\n",
    "                                      feed_dict={X:self.x_validation,\n",
    "                                                 Y:self.y_validation})\n",
    "                    self.accuracy.append(result[0])\n",
    "                    print('Pass number: ',pass_i,\n",
    "                          ' -- validation set accuracy: ',result[0])\n",
    "            # save cost and result lists for examining model performance\n",
    "            self._cost_list = cost_list\n",
    "            self._result_list = sess.run([tf.arg_max(probs_x,1)],\n",
    "                                         feed_dict={X:self.x_test,\n",
    "                                                    Y:self.y_test})\n",
    "            # save model in self._save_path\n",
    "            save_path = saver.save(sess,self._save_path)\n",
    "            print(\"Model saved in file: {}\".format(save_path))\n",
    "\n",
    "    def check_results(self):\n",
    "        \"\"\"\n",
    "        Prints a confusion matrix of performance on the test set,\n",
    "        and instantiates lists of True Positive, True Negative,\n",
    "        False Positive, and False Negative for inspection as\n",
    "        self._TP, self._TN, self._FP, self._FN.\n",
    "        \"\"\"\n",
    "\n",
    "        # print confusion matrix\n",
    "        true_y_labels = np.array(self.y_test[:,1])\n",
    "        print('\\t\\tPredicted:')\n",
    "        print('\\t\\tmin. maj.')\n",
    "        print('Actual:\\t min.',\n",
    "              confusion_matrix(true_y_labels,self._result_list[0])[0])\n",
    "        print('    \\t maj.',\n",
    "              confusion_matrix(true_y_labels,self._result_list[0])[1])\n",
    "\n",
    "        # identify predicted and true positives and negatives\n",
    "        predicted_pos = np.where(self._result_list[0]==1)\n",
    "        predicted_neg = np.where(self._result_list[0]==0)\n",
    "        actual_pos = np.where(np.argmax(self.y_test,1)==1)\n",
    "        actual_neg = np.where(np.argmax(self.y_test,1)==0)\n",
    "\n",
    "        # indices of shuffled and split data (just y_test)\n",
    "        true_pos = np.intersect1d(predicted_pos,actual_pos).tolist()\n",
    "        true_neg = np.intersect1d(predicted_neg,actual_neg).tolist()\n",
    "        false_pos = np.intersect1d(predicted_pos,actual_neg).tolist()\n",
    "        false_neg = np.intersect1d(predicted_neg,actual_pos).tolist()\n",
    "        y_indices = self.shuffled_idx[self._validation_test_split_idx:\n",
    "                                      self._train_test_split_idx]\n",
    "\n",
    "        # create lists of true and false positives and negatives\n",
    "        self._TP = [list(self.raw_X[y_indices[i]]) for i in true_pos]\n",
    "        self._TN = [list(self.raw_X[y_indices[i]]) for i in true_neg]\n",
    "        self._FP = [list(self.raw_X[y_indices[i]]) for i in false_pos]\n",
    "        self._FN = [list(self.raw_X[y_indices[i]]) for i in false_neg]\n",
    "\n",
    "    def predict(self,csv_file):\n",
    "        \"\"\"\n",
    "        Predicts the type of error between the two strings in each row of\n",
    "        a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            0 for minor, 1 for major,\n",
    "            'No error' for identical strings,\n",
    "            and 'Unknown' if a prediction cannot be made (could change to 0).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        # build graph and initialize session\n",
    "        tf.reset_default_graph()\n",
    "        (X,_),_,_,_,pred_y,lr,saver = self._build_graph(training=False)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver.restore(sess,self._save_path)\n",
    "\n",
    "            # generate calculations from 2d array of input strings\n",
    "            for row in np.genfromtxt(csv_file,dtype='str',delimiter=','):\n",
    "                str_1,str_2 = row[0],row[1]\n",
    "\n",
    "                # strings identical\n",
    "                if str_1 == str_2:\n",
    "                    predictions.append('No error')\n",
    "                    continue\n",
    "\n",
    "                # model prediction\n",
    "                try:\n",
    "                    pred = sess.run([tf.arg_max(pred_y,1)],\n",
    "                                    feed_dict=\\\n",
    "                                    {X: self._data_generator(str_1,str_2)})\n",
    "                    predictions.append(str(pred[0][0]+1))\n",
    "\n",
    "                # can't predict\n",
    "                except:\n",
    "                    predictions.append('Unknown')\n",
    "        \n",
    "        print(','.join(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = error_checker('/Users/Rutherford/Desktop/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this  isn't  anything  to  aweome write  home  'bout but here goez nuttin'\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puncs2 = '[!@#$%^&*()_+=:;\"/,`.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_list = [' '.join(sent) for sent in sentences[:100]]\n",
    "clean_sents = [re.sub(\"''\",'',almost) for almost in [re.sub(puncs2,'',sent) for sent in sent_list]]\n",
    "with open('/Users/Rutherford/Desktop/sents.txt','w') as f:\n",
    "    for sent in clean_sents:\n",
    "        f.write('1,{}\\n'.format(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_sent_list = [' '.join(sent) for sent in sentences[100:150]]\n",
    "clean_bad_sents = [re.sub(\"''\",'',almost) for almost in [re.sub(puncs2,'',sent) for sent in bad_sent_list]]\n",
    "with open('/Users/Rutherford/Desktop/bad_sents.txt','w') as f:\n",
    "    for sent in clean_bad_sents:\n",
    "        f.write('2,{}\\n'.format(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1,Y1,indices1,X_in1 = classifier._train_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[135,\n",
       " 115,\n",
       " 131,\n",
       " 55,\n",
       " 95,\n",
       " 29,\n",
       " 156,\n",
       " 51,\n",
       " 101,\n",
       " 145,\n",
       " 19,\n",
       " 85,\n",
       " 15,\n",
       " 66,\n",
       " 24,\n",
       " 30,\n",
       " 132,\n",
       " 105,\n",
       " 151,\n",
       " 16,\n",
       " 75,\n",
       " 18,\n",
       " 12,\n",
       " 9,\n",
       " 31,\n",
       " 154,\n",
       " 98,\n",
       " 56,\n",
       " 134,\n",
       " 159,\n",
       " 139,\n",
       " 78,\n",
       " 60,\n",
       " 84,\n",
       " 2,\n",
       " 94,\n",
       " 45,\n",
       " 42,\n",
       " 69,\n",
       " 152,\n",
       " 26,\n",
       " 141,\n",
       " 117,\n",
       " 93,\n",
       " 133,\n",
       " 36,\n",
       " 82,\n",
       " 22,\n",
       " 126,\n",
       " 67,\n",
       " 97,\n",
       " 11,\n",
       " 65,\n",
       " 86,\n",
       " 6,\n",
       " 27,\n",
       " 76,\n",
       " 142,\n",
       " 38,\n",
       " 41,\n",
       " 4,\n",
       " 138,\n",
       " 32,\n",
       " 144,\n",
       " 109,\n",
       " 68,\n",
       " 10,\n",
       " 96,\n",
       " 111,\n",
       " 0,\n",
       " 122,\n",
       " 123,\n",
       " 64,\n",
       " 44,\n",
       " 146,\n",
       " 28,\n",
       " 40,\n",
       " 114,\n",
       " 25,\n",
       " 23,\n",
       " 119,\n",
       " 81,\n",
       " 79,\n",
       " 39,\n",
       " 90,\n",
       " 108,\n",
       " 158,\n",
       " 137,\n",
       " 47,\n",
       " 124,\n",
       " 61,\n",
       " 73,\n",
       " 33,\n",
       " 112,\n",
       " 120,\n",
       " 128,\n",
       " 62,\n",
       " 161,\n",
       " 100,\n",
       " 104,\n",
       " 53,\n",
       " 5,\n",
       " 118,\n",
       " 127,\n",
       " 150,\n",
       " 49,\n",
       " 35,\n",
       " 80,\n",
       " 77,\n",
       " 34,\n",
       " 46,\n",
       " 7,\n",
       " 43,\n",
       " 70,\n",
       " 125,\n",
       " 110,\n",
       " 91,\n",
       " 83,\n",
       " 147,\n",
       " 148,\n",
       " 89,\n",
       " 8,\n",
       " 155,\n",
       " 113,\n",
       " 13,\n",
       " 59,\n",
       " 140,\n",
       " 3,\n",
       " 17,\n",
       " 72,\n",
       " 143,\n",
       " 136,\n",
       " 149,\n",
       " 63,\n",
       " 54,\n",
       " 107,\n",
       " 50,\n",
       " 160,\n",
       " 58,\n",
       " 48,\n",
       " 88,\n",
       " 21,\n",
       " 57,\n",
       " 157,\n",
       " 129,\n",
       " 37,\n",
       " 153,\n",
       " 1,\n",
       " 52,\n",
       " 130,\n",
       " 103,\n",
       " 99,\n",
       " 116,\n",
       " 87,\n",
       " 74,\n",
       " 121,\n",
       " 162,\n",
       " 20,\n",
       " 71,\n",
       " 106,\n",
       " 14,\n",
       " 92,\n",
       " 102]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_validation_test_split_idx = int(len(Y1)*.3)\n",
    "# split data\n",
    "x_validation = X1[:_validation_test_split_idx]\n",
    "x_train = X1[_validation_test_split_idx:]\n",
    "y_validation = Y1[:_validation_test_split_idx]\n",
    "y_train = Y1[_validation_test_split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceClassification:\n",
    "\n",
    "    def __init__(self, data, target, num_hidden=200, num_layers=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property    \n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        output, _ = tf.nn.dynamic_rnn(\n",
    "            tf.nn.rnn_cell.GRUCell(self._num_hidden),\n",
    "            data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "        # Softmax layer.\n",
    "        weight, bias = self._weight_and_bias(\n",
    "            self._num_hidden, int(self.target.get_shape()[1]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        #prediction = tf.arg_max(tf.nn.softmax(tf.matmul(last, weight) + bias),1)\n",
    "        \n",
    "        ### predictions and accuracy\n",
    "        ##correct_prediction = tf.equal(tf.arg_max(probs_x,1),tf.arg_max(Y,1))\n",
    "        ##accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        #learning_rate = 0.003\n",
    "        #optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        learning_rate = 30\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_words = max(map(lambda x:len(x.split()),X_in1))\n",
    "\n",
    "lis = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    lis.append(np.vstack((x_train[i],np.zeros([most_words-x_train[i].shape[0],300]))))\n",
    "new_x_train = np.asarray(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lis = []\n",
    "for i in range(x_validation.shape[0]):\n",
    "    lis.append(np.vstack((x_validation[i],np.zeros([most_words-x_validation[i].shape[0],300]))))\n",
    "new_x_validation = np.asarray(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 52, 300)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 52, 300)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33128834355828218"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(Y1,1))/float(len(Y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 35.4%\n",
      "Epoch  2 error 35.4%\n",
      "Epoch  3 error 35.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-504-6514a027ca93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(batch_x.shape,batch_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m###print(sess.run(model.prediction,{data: batch_x, target: batch_y}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#sess.run(model.optimize, {data: batch_x[0].reshape([1,-1,300]), target: batch_y[0]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rutherford/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "rows, row_size = new_x_train.shape[1],new_x_train.shape[2]\n",
    "num_classes = 2\n",
    "data = tf.placeholder(tf.float32, [None, rows, row_size])\n",
    "target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "model = VariableSequenceClassification(data, target)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for epoch in range(10):\n",
    "    for _ in range(100):\n",
    "        # take 75 random train examples per run\n",
    "        idxs = np.random.randint(0,new_x_train.shape[0],75)\n",
    "        #batch_x = x_train[idxs]\n",
    "        #batch_y = y_train[idxs]\n",
    "        batch_x = new_x_train[idxs]\n",
    "        batch_y = y_train[idxs]\n",
    "        #print(batch_x.shape,batch_y.shape)\n",
    "        sess.run(model.optimize, {data: batch_x, target: batch_y})\n",
    "        ###print(sess.run(model.prediction,{data: batch_x, target: batch_y}))\n",
    "        #sess.run(model.optimize, {data: batch_x[0].reshape([1,-1,300]), target: batch_y[0]})\n",
    "    error = sess.run(model.error, {data: new_x_validation, target: y_validation})\n",
    "    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error))\n",
    "    # i'm so glad i've trained it to get dumber!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fGRU(iSize, hSize, nameScope='Recursion'):\n",
    "    '''\n",
    "        :param iSize: Dimensionality of input sequence elements - not the sequence length\n",
    "        but the length of each element in the sequence.\n",
    "        :param hSize: Hidden layer size\n",
    "        :return: fR uses the input parameters to define weight and bias functions and\n",
    "        then returns the GRU recursion function in form useable by tf scan function\n",
    "        '''\n",
    "    with tf.name_scope(nameScope) as scope:\n",
    "\n",
    "        #Random Initialization\n",
    "        sd = 0.01\n",
    "        Wr = tf.Variable(tf.truncated_normal([iSize, hSize], stddev=sd), name=\"Wx\")\n",
    "        Ur = tf.Variable(tf.truncated_normal([hSize, hSize], stddev=sd), name='Ur')\n",
    "        Wz = tf.Variable(tf.truncated_normal([iSize, hSize], stddev=sd), name=\"Wz\")\n",
    "        Uz = tf.Variable(tf.truncated_normal([hSize, hSize], stddev=sd), name='Uz')\n",
    "        Wh = tf.Variable(tf.truncated_normal([iSize, hSize], stddev=sd), name=\"Wh\")\n",
    "        Uh = tf.Variable(tf.truncated_normal([hSize, hSize], stddev=sd), name='Uh')\n",
    "\n",
    "        br = tf.Variable(tf.zeros([hSize]), name=\"br\")\n",
    "        bz = tf.Variable(tf.zeros([hSize]), name=\"bz\")\n",
    "        bh = tf.Variable(tf.zeros([hSize]), name=\"bh\")\n",
    "\n",
    "        def fR(htm1, x):\n",
    "            zt = tf.sigmoid(tf.matmul(htm1,Uz) + tf.matmul(x, Wz) + bz)\n",
    "            rt = tf.sigmoid(tf.matmul(htm1,Ur) + tf.matmul(x, Wr) + br)\n",
    "            hSquig = tf.tanh(tf.matmul(htm1,Uh) + rt*tf.matmul(x,Wh) + bh)\n",
    "            current_hidden_state = (1 - zt) * htm1 + zt * hSquig\n",
    "            return current_hidden_state\n",
    "    return fR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#replace with placeholders\n",
    "input_size = 300 #1\n",
    "hidden_size = 10 # changed from 32\n",
    "target_size = 2\n",
    "\n",
    "# Weights for output layers\n",
    "Wo = tf.Variable(tf.truncated_normal([hidden_size, target_size], mean=0, stddev=.01), name=\"Wo\")\n",
    "bo = tf.Variable(tf.truncated_normal([target_size], mean=0, stddev=.01), name=\"bo\")\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, None, input_size], name='inputs')\n",
    "inputs_ = tf.transpose(inputs, perm=[1,0,2])\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, target_size])\n",
    "\n",
    "initial_hidden = inputs[:, 0, :]\n",
    "initial_hidden = tf.matmul(initial_hidden, tf.zeros([input_size, hidden_size]))\n",
    "\n",
    "#define recursion for scan function\n",
    "#fRScan = fElman(inputSize, hiddenSize)\n",
    "#fRScan = mlElman(inputSize, hiddenSize, 1)\n",
    "fRScan = fGRU(input_size, hidden_size)\n",
    "\n",
    "#hidden state sequence\n",
    "all_hidden_states = tf.scan(fRScan, inputs_, initializer=initial_hidden, name=\"RNNStates\")\n",
    "\n",
    "#extract last hidden state\n",
    "last_hidden_state = tf.reverse(all_hidden_states, [True, False, False])[0, :, :]\n",
    "\n",
    "#develop logit from last hidden state\n",
    "last_output = tf.matmul(last_hidden_state, Wo) + bo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows_of_cost = tf.nn.softmax_cross_entropy_with_logits(last_output,y,\n",
    "                                            name='rows_of_cost')\n",
    "#    average over all pairs\n",
    "cost = tf.reduce_mean(rows_of_cost,reduction_indices=None,\n",
    "                      keep_dims=False,name='cost')\n",
    "\n",
    "\n",
    "\n",
    "#loss definition for gradients\n",
    "loss = tf.reduce_mean(tf.square(y - last_output))\n",
    "\n",
    "#rms error for printing\n",
    "rms_error = tf.sqrt(loss)\n",
    "\n",
    "#training function\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for epoch in range(int(len(indices1)*.8)-1):  # 120 ... 4096\n",
    "        #X_train, y_train = data_gen(100, 10)\n",
    "        #X_test, y_test = data_gen(100, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ts, lhs = sess.run([train_step, last_hidden_state], \n",
    "                           {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])})\n",
    "\n",
    "\n",
    "        Loss = str(sess.run(loss, {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])}))\n",
    "        Train_error = str(sess.run(rms_error, {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])}))\n",
    "        Test_error = str(sess.run(rms_error, {inputs: x_validation[epoch//int(len(indices1)*.2)-1].reshape([1,-1,300]), y: y_validation[epoch//int(len(indices1)*.2)-1].reshape([1,2])}))\n",
    "\n",
    "        print(\"\\rIteration: %s Loss: %s Train Error: %s Test Error: %s\" %\n",
    "              (epoch, Loss, Train_error, Test_error)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for epoch in range(int(len(indices1)*.8)-1):  # 120 ... 4096\n",
    "        #X_train, y_train = data_gen(100, 10)\n",
    "        #X_test, y_test = data_gen(100, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ts, lhs = sess.run([train_step, last_hidden_state], \n",
    "                           {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])})\n",
    "\n",
    "\n",
    "        Loss = str(sess.run(loss, {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])}))\n",
    "        Train_error = str(sess.run(rms_error, {inputs: x_train[epoch].reshape([1,-1,300]), y: y_train[epoch].reshape([1,2])}))\n",
    "        Test_error = str(sess.run(rms_error, {inputs: x_validation[epoch//int(len(indices1)*.2)-1].reshape([1,-1,300]), y: y_validation[epoch//int(len(indices1)*.2)-1].reshape([1,2])}))\n",
    "\n",
    "        print(\"\\rIteration: %s Loss: %s Train Error: %s Test Error: %s\" %\n",
    "              (epoch, Loss, Train_error, Test_error)),\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
